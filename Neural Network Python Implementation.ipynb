{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a simple Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network of 3 layers, with 2 neurons in input layer, 3 neurons in hidden layer, 2 neurons in output layer\n",
    "- Implementation of Backpropagation algorithm with Gradient Descent\n",
    "- Learning optimisation by using Matrices and Vectors to represent data in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation using Matrices and Vectors\n",
    "Instead of training the network by updating each individual weight and bias seperately, we can optimise the learning time required by representing our data in matrices and vectors. This will allow the network to simultaneously update the weights and biases in each layer. As the number of layers and neurons increases, the optimisation of the training time becomes more evident when using matrices and vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Matrix/Vector form, the sum of the weighted input Z<sup>l</sup> becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z<sup>l</sup> = W<sup>l</sup>a<sup>l-1</sup> + b<sup>l</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Weight Matrix:</u> <b>W<sup>l</sup> between layer l and l-1 contains a ROW for each neuron in layer l-1, and a COLUMN for each neuron in layer l</b>\n",
    "\n",
    "For example, our network is (2,3,2) - weight matrix for connections between hidden layer and input layer will have 2 rows and 3 columns (2x3)\n",
    "\n",
    "weight matrix for connections between output layer and hidden layer will have 3 rows and 2 columns (3x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Activation Vector:</u> <b>a<sup>l-1</sup> is a vector which contains one entry for each neuron in layer l-1. These entries are the activation of the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Bias Vector:</u> <b>b<sup>l</sup> is a vector which contains one entry for each neuron in layer l. These entries are the bias for that neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Sum of the Weighted Input Vector Z<sup>l</sup>:</u> Z<sup>l</sup> is a vector which contains one entry for each neuron in layer l. These entries are the sum of the weighted inputs, plus the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Activation a<sup>l</sup> is then an activation function applied to EVERY ENTRY in the vector Z<sup>l</sup>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a<sup>l</sup> =  Ïƒ(z<sup>l</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why initialise NN weights as random numbers?\n",
    "\n",
    "During forward propagation, each unit in the hidden layer gets the sum of the input multipled by the corresponding weight. If all of these weights were initialised to the same value (e.g. zero or one) then each neuron would get exactly the same signal. \n",
    "\n",
    "For example, if all weights were initialized to 1, each neuron gets a signal equal to sum of inputs (and outputs sigmoid(sum(inputs))). If all weights are 0, which is even worse, every neuron will get a zero signal. No matter what was the input - if all weights are the same, all neurons in the hidden layer will be the same too.\n",
    "\n",
    "This is the main issue with symmetry and reason why you should initialize weights randomly (or, at least, with different values). \n",
    "\n",
    "<b>If all weights start with equal values and if the solution requires that unequal weights be developed, the system can never learn.</b>\n",
    "\n",
    "This is because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units must always be the same. The system is starting out at a kind of unstable equilibrium point that keeps the weights equal, but it is higher than some neighboring points on the error surface, and once it moves away to one of these points, it will never return. We counteract this problem by starting the system with small random weights. Under these conditions symmetry problems of this kind do not arise.\n",
    "\n",
    "As to why between 0 and 1. The inputs to our neural network need to be normalized to a common range so they can be compared sensibly.  Consider the confusion if one input ranged between 300 and 700 and another ranged between 200 and 85000, and the value of both was 450. If we don't normalize the two ranges, it's difficult to determine which input is more severe.\n",
    "\n",
    "For weight initialisations, its common to use small random Gaussians with mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python implementation of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid Activation func.\n",
    "- SSE for Cost function\n",
    "- Learning Rate of 0.5\n",
    "- Random initial weights\n",
    "\n",
    "Forwards Pass\n",
    "- Evaluate the performance of the network with its current values for Weights/Biases and input/output train patterns\n",
    "\n",
    "Backwards Pass\n",
    "- Find derivative of Cost function with respect to Weights/Biases and update values to reduce cost (Grad. Descent)\n",
    "- Superscript 'L' denotes OUTPUT LAYER\n",
    "\n",
    "$$COST(y, a^L) = \\sum_{i=1}^n \\frac{1}{2}(y - a^L)^2$$\n",
    "\n",
    "\n",
    "OUTPUT LAYER\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial COST(y, a^L)}{\\partial W^L} =  \\frac{\\partial COST(y, a^L)}{\\partial a^L} * \\frac{\\partial a^L}{\\partial z^L} * \\frac{\\partial z^L}{\\partial W^L} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "= (a^L - y) * a^{L}(1-a^{L}) * a^{L-1}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HIDDEN LAYER\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial COST(y, a^L)}{\\partial W^{L-1}} =  \\frac{\\partial COST(y, a^L)}{\\partial a^L} * \\frac{\\partial a^L}{\\partial z^L} * \\frac{\\partial z^L}{\\partial a^{L-1}}  * \\frac{\\partial a^{L-1}}{\\partial z^{L-1}} * \\frac{\\partial z^{L-1}}{\\partial W^{L-1}} \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "= (a^L - y) * a^{L}(1-a^{L}) * W^L * a^{L-1}(1-a^{L-1}) * a^{L-2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights between input and hidden:\n",
      "\n",
      "[[ 0.65466267  0.34384017  0.08110898]\n",
      " [ 0.52952613  0.18642881  0.48497102]]\n",
      "\n",
      "initial biases for hidden layer:\n",
      "\n",
      "[0, 0, 0]\n",
      "\n",
      "initial weights between hidden and output:\n",
      "\n",
      "[[ 0.03518685  0.29357741]\n",
      " [ 0.56166218  0.04892028]\n",
      " [ 0.04586889  0.24855736]]\n",
      "\n",
      "initial biases for output layer:\n",
      "\n",
      "[0, 0]\n",
      "\n",
      "---------- Network Train Performance ----------\n",
      "Target output: \n",
      "[[ 0.01  0.99]\n",
      " [ 0.03  0.8 ]\n",
      " [ 0.04  0.7 ]\n",
      " [ 0.06  0.75]]\n",
      "\n",
      "Network output: \n",
      "[[ 0.00999997  0.98999995]\n",
      " [ 0.03        0.8       ]\n",
      " [ 0.04        0.7       ]\n",
      " [ 0.06        0.75      ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.y = y\n",
    "    \n",
    "        self.lRate = 0.5\n",
    "        \n",
    "        # Initialise weight matrices np.random.rand chooses random numbers between 0 and 1\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],3)\n",
    "        self.bias1 = [0,0,0]\n",
    "        \n",
    "        self.weights2 = np.random.rand(3,2)\n",
    "        self.bias2 = [0,0]\n",
    "        \n",
    "        # Print out initial weights and biases\n",
    "        print(\"initial weights between input and hidden:\" + \"\\n\")\n",
    "        print(str(self.weights1) + \"\\n\")\n",
    "        print(\"initial biases for hidden layer:\" + \"\\n\")\n",
    "        print(str(self.bias1) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"initial weights between hidden and output:\" + \"\\n\")\n",
    "        print(str(self.weights2) + \"\\n\")\n",
    "        print(\"initial biases for output layer:\" + \"\\n\")\n",
    "        print(str(self.bias2) + \"\\n\")\n",
    "   \n",
    "    \n",
    "        # Network output should have same shape as our 'y' output so we can calc. cost\n",
    "        self.output = np.zeros(y.shape)\n",
    "        \n",
    "        \n",
    "               \n",
    "        \n",
    "    # FORWARD PASS: Evaluating network by calculating activations at each layer (using SIGMOID activation function)\n",
    "    def feedforward(self):\n",
    "        \n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1)+self.bias1)\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2)+self.bias2)\n",
    "        \n",
    "        \n",
    "    # BACKWARD PASS: Updating values of Weights and Biases by using Gradient Descent techniques\n",
    "    def backpropagation(self):\n",
    "            \n",
    "        # chain rule to find derivative of the cost function with respect to weights2/bias2 and weights1/bias1\n",
    "        d_weights2 = np.dot(self.layer1.T, ((self.output - self.y) * sigmoid_derivative(self.output)))\n",
    "        d_bias2 =((self.output - self.y) * sigmoid_derivative(self.output))\n",
    "        \n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot((self.output - self.y) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "        d_bias1 = (np.dot((self.output - self.y) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1))\n",
    "            \n",
    "        # update weights\n",
    "        self.weights1 -= self.lRate * d_weights1\n",
    "        self.weights2 -= self.lRate * d_weights2\n",
    "            \n",
    "        # update biases\n",
    "        self.bias1 = self.bias1 - (self.lRate * d_bias1)\n",
    "        self.bias2 = self.bias2 - (self.lRate * d_bias2)\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[0.05,0.1],\n",
    "                  [0.07,0.11],\n",
    "                  [0.02,0.2],\n",
    "                  [0.01,0.15]])\n",
    "    \n",
    "    y = np.array([[0.01,0.99],[0.03, 0.8],[0.04, 0.7],[0.06, 0.75]])\n",
    "    nn = NeuralNetwork(X,y)\n",
    "    iterations = 150000\n",
    "\n",
    "    for i in range(iterations):\n",
    "        nn.feedforward()\n",
    "        nn.backpropagation()\n",
    "\n",
    "    print('---------- Network Train Performance ----------')  \n",
    "    print(\"Target output: \" + \"\\n\" + str(y) + \"\\n\\n\" + \"Network output: \" + \"\\n\" + str(nn.output))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
